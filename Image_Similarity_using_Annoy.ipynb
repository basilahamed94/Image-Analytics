{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7f7M0ezZhBS",
        "outputId": "d8abcffb-f060-4846-9bfd-896704413dae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNL7asldpCet",
        "outputId": "b51b6a04-950a-4331-b126-c43d42e6c7c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.7.13 (default, Apr 24 2022, 01:04:09) \n",
            "[GCC 7.5.0]\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(sys.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln7u_4m5RIV_"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJuCDioUcH8B",
        "outputId": "38dc9ebb-3606-40c2-a965-16fabc5286f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting DeepImageSearch\n",
            "  Downloading DeepImageSearch-1.4.tar.gz (6.5 kB)\n",
            "Collecting annoy\n",
            "  Downloading annoy-1.17.0.tar.gz (646 kB)\n",
            "\u001b[K     |████████████████████████████████| 646 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from DeepImageSearch) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from DeepImageSearch) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from DeepImageSearch) (1.21.6)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from DeepImageSearch) (2.8.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from DeepImageSearch) (4.64.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from DeepImageSearch) (7.1.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->DeepImageSearch) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->DeepImageSearch) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->DeepImageSearch) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->DeepImageSearch) (3.0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->DeepImageSearch) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->DeepImageSearch) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->DeepImageSearch) (2022.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->DeepImageSearch) (1.6.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->DeepImageSearch) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->DeepImageSearch) (0.2.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->DeepImageSearch) (0.5.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->DeepImageSearch) (3.3.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->DeepImageSearch) (14.0.1)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->DeepImageSearch) (2.8.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->DeepImageSearch) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->DeepImageSearch) (0.25.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->DeepImageSearch) (3.17.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->DeepImageSearch) (1.1.2)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->DeepImageSearch) (2.8.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->DeepImageSearch) (2.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->DeepImageSearch) (1.0.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->DeepImageSearch) (1.14.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 57.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->DeepImageSearch) (1.44.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->DeepImageSearch) (1.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->DeepImageSearch) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->DeepImageSearch) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->DeepImageSearch) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->DeepImageSearch) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->DeepImageSearch) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->DeepImageSearch) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->DeepImageSearch) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->DeepImageSearch) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->DeepImageSearch) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->DeepImageSearch) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->DeepImageSearch) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->DeepImageSearch) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->DeepImageSearch) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->DeepImageSearch) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->DeepImageSearch) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->DeepImageSearch) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->DeepImageSearch) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->DeepImageSearch) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->DeepImageSearch) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->DeepImageSearch) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->DeepImageSearch) (3.2.0)\n",
            "Building wheels for collected packages: DeepImageSearch, annoy\n",
            "  Building wheel for DeepImageSearch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for DeepImageSearch: filename=DeepImageSearch-1.4-py3-none-any.whl size=7412 sha256=66ef0421200df1ce792b1f9f7cbc520f07cf1c02921a783ee0ad24620cf43a85\n",
            "  Stored in directory: /root/.cache/pip/wheels/36/f6/97/563fa2462d6f4ef5a80c3f7be55d54a47850c0fc98340b1d5d\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.0-cp37-cp37m-linux_x86_64.whl size=391602 sha256=64e1f258adc62999c43e04065cd5d9bdccbd63f92bab8570289035985e3ffe4c\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/e8/1e/7cc9ebbfa87a3b9f8ba79408d4d31831d67eea918b679a4c07\n",
            "Successfully built DeepImageSearch annoy\n",
            "Installing collected packages: tf-estimator-nightly, annoy, DeepImageSearch\n",
            "Successfully installed DeepImageSearch-1.4 annoy-1.17.0 tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        }
      ],
      "source": [
        "pip install DeepImageSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drzdXBlUZAsG"
      },
      "outputs": [],
      "source": [
        "from sys import stdout\n",
        "from time import sleep\n",
        "import DeepImageSearch.config as config\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from annoy import AnnoyIndex\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "class LoadData:\n",
        "    \"\"\"Loading the data from Single/Multiple Folders or form CSV file\"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def from_folder(self,folder_list:list): # Enter the Single Folder Path/List of the Folders\n",
        "        self.folder_list = folder_list\n",
        "        image_path = []\n",
        "        for folder in self.folder_list:\n",
        "            for path in os.listdir(folder):\n",
        "                image_path.append(os.path.join(folder,path))\n",
        "        return image_path # Returning list of images\n",
        "    def from_csv(self,csv_file_path:str,images_column_name:str): # CSV File path with Images path Columns Name\n",
        "        self.csv_file_path = csv_file_path\n",
        "        self.images_column_name = images_column_name\n",
        "        return pd.read_csv(self.csv_file_path)[self.images_column_name].to_list() # Returning list of images\n",
        "\n",
        "class FeatureExtractor:\n",
        "    def __init__(self):\n",
        "        # Use VGG-16 as the architecture and ImageNet for the weight\n",
        "        base_model = VGG16(weights='imagenet')\n",
        "        # Customize the model to return features from fully-connected layer\n",
        "        self.model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc1').output)\n",
        "    def extract(self, img):\n",
        "        # Resize the image\n",
        "        img = img.resize((224, 224))\n",
        "        # Convert the image color space\n",
        "        img = img.convert('RGB')\n",
        "        # Reformat the image\n",
        "        x = image.img_to_array(img)\n",
        "        x = np.expand_dims(x, axis=0)\n",
        "        x = preprocess_input(x)\n",
        "        # Extract Features\n",
        "        feature = self.model.predict(x)[0]\n",
        "        return feature / np.linalg.norm(feature)\n",
        "    def get_feature(self,image_data:list):\n",
        "        self.image_data = image_data \n",
        "        #fe = FeatureExtractor()\n",
        "        features = []\n",
        "        for img_path in tqdm(self.image_data): # Iterate through images \n",
        "            # Extract Features\n",
        "            try:\n",
        "                feature = self.extract(img=Image.open(img_path))\n",
        "                features.append(feature)\n",
        "            except:\n",
        "                features.append(None)\n",
        "                continue\n",
        "        return features\n",
        "\n",
        "class Index:\n",
        "    def __init__(self,image_list:list):\n",
        "        self.image_list = image_list\n",
        "        if 'meta-data-files' not in os.listdir():\n",
        "            os.makedirs(\"meta-data-files\")\n",
        "        self.FE = FeatureExtractor()\n",
        "    def start_feature_extraction(self):\n",
        "        image_data = pd.DataFrame()\n",
        "        image_data['images_paths'] = self.image_list\n",
        "        f_data = self.FE.get_feature(self.image_list)\n",
        "        image_data['features']  = f_data\n",
        "        image_data = image_data.dropna().reset_index(drop=True)\n",
        "        image_data.to_pickle(config.image_data_with_features_pkl)\n",
        "        print(\"Image Meta Information Saved: [meta-data-files/image_data_features.pkl]\")\n",
        "        return image_data\n",
        "    def start_indexing(self,image_data):\n",
        "        self.image_data = image_data\n",
        "        f = len(image_data['features'][0]) # Length of item vector that will be indexed\n",
        "        t = AnnoyIndex(f, 'euclidean')\n",
        "        for i,v in tqdm(zip(self.image_data.index,image_data['features'])):\n",
        "            t.add_item(i, v)\n",
        "        t.build(100) # 100 trees\n",
        "        print(\"Saved the Indexed File:\"+\"[meta-data-files/image_features_vectors.ann]\")\n",
        "        t.save(config.image_features_vectors_ann)\n",
        "    def Start(self):\n",
        "        if len(os.listdir(\"meta-data-files/\"))==0:\n",
        "            data = self.start_feature_extraction()\n",
        "            self.start_indexing(data)\n",
        "        else:\n",
        "            print(\"Metadata and Features are allready present, Do you want Extract Again? Enter yes or no\")\n",
        "            flag  = str(input())\n",
        "            if flag.lower() == 'yes':\n",
        "                data = self.start_feature_extraction()\n",
        "                self.start_indexing(data)\n",
        "            else:\n",
        "                print(\"Meta data allready Present, Please Apply Search!\")\n",
        "                print(os.listdir(\"meta-data-files/\"))\n",
        "\n",
        "class SearchImage:\n",
        "    def __init__(self,image_data):\n",
        "        self.image_data = image_data\n",
        "        self.f = len(self.image_data['features'][0])\n",
        "    def search_by_vector(self,v,n:int):\n",
        "        self.v = v # Feature Vector\n",
        "        self.n = n # number of output \n",
        "        u = AnnoyIndex(self.f, 'euclidean')\n",
        "        u.load(config.image_features_vectors_ann) # super fast, will just mmap the file\n",
        "        index_list = u.get_nns_by_vector(self.v, self.n, include_distances=True) # will find the 10 nearest neighbors\n",
        "        #return index_list[0]\n",
        "        s = [i[39:] for i in self.image_data.iloc[index_list[0]]['images_paths'].to_list()]\n",
        "        df = pd.DataFrame(list(zip(index_list[0],s,index_list[1])),columns=['Index','Image Path','Similarity'])\n",
        "        df = df[df['Similarity']<=1]\n",
        "        return df\n",
        "    def get_query_vector(self,image_path:str):\n",
        "        self.image_path = image_path\n",
        "        img = Image.open(self.image_path)\n",
        "        fe = FeatureExtractor()\n",
        "        query_vector = fe.extract(img)\n",
        "        return query_vector\n",
        "    def plot_similar_images(self,image_path:str):\n",
        "        self.image_path = image_path\n",
        "        query_vector = self.get_query_vector(self.image_path)\n",
        "        img_list = list(self.search_by_vector(query_vector,16).values())\n",
        "        # Visualize the result\n",
        "        axes=[]\n",
        "        fig=plt.figure(figsize=(20,15))\n",
        "        for a in range(4*4):\n",
        "            axes.append(fig.add_subplot(4, 4, a+1))  \n",
        "            plt.axis('off')\n",
        "            plt.imshow(Image.open(img_list[a]))\n",
        "        fig.tight_layout()\n",
        "        fig.suptitle('Similar Result Found', fontsize=22)\n",
        "        plt.show(fig)\n",
        "    def get_similar_images(self,image_path:str,number_of_images:int):\n",
        "        self.image_path = image_path\n",
        "        self.number_of_images = number_of_images\n",
        "        query_vector = self.get_query_vector(self.image_path)\n",
        "        img_dict = self.search_by_vector(query_vector,self.number_of_images)\n",
        "        return img_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkrqCbxwcUJ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef0cf1e2-ef57-4e9b-a3f8-a85200d5a1dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47544"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "image_list = [ '/content/drive/My Drive/HP_PMAY_IMAGES/'+i for i in os.listdir('/content/drive/My Drive/HP_PMAY_IMAGES/')] \n",
        "len(image_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkInYu3lcezq",
        "outputId": "3d4ec8cb-8a39-4cdc-be48-736464e73396"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metadata and Features are allready present, Do you want Extract Again? Enter yes or no\n",
            "yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 23%|██▎       | 576/2544 [08:32<27:49,  1.18it/s]"
          ]
        }
      ],
      "source": [
        "Index(image_list[45000:]).Start()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = pd.DataFrame()\n",
        "for i in range(1,9):\n",
        "  file = 'image_data_features_'+str(i)+'.pkl'\n",
        "  a = pd.read_pickle(os.path.join('meta-data-files/',file))\n",
        "  print(a.shape)\n",
        "  res = res.append(a,ignore_index = True)\n",
        "print('Final Shape',res.shape)"
      ],
      "metadata": {
        "id": "z3KstGScsZ6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_data = res\n",
        "f = len(image_data.features[0]) # Length of item vector that will be indexed\n",
        "t = AnnoyIndex(f, 'euclidean')\n",
        "for i,v in tqdm(zip(image_data.index,image_data['features'])):\n",
        "    t.add_item(i, v)\n",
        "t.build(100) # 100 trees\n",
        "print(\"Saved the Indexed File:\"+\"[meta-data-files/image_features_vectors.ann]\")\n",
        "t.save(config.image_features_vectors_ann)"
      ],
      "metadata": {
        "id": "GaMrRFj9sgtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-HSPEBxdnno",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4936a946-8dc9-49e5-8450-704ff360fcff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100.0% Completed --|-- Result Shape : (35734, 4) --|-- Errors: 0 "
          ]
        }
      ],
      "source": [
        "res_df = pd.DataFrame(columns = ['Index','Image Path','Similarity','Query Image'])\n",
        "k=0\n",
        "err = []\n",
        "#image_data = pd.read_pickle(config.image_data_with_features_pkl)\n",
        "i_list = image_data.images_paths[40000:]\n",
        "# for j in range(0,47540,5):\n",
        "#   i_list = image_data.images_paths[j:j+20]\n",
        "for ind,i in enumerate(i_list):\n",
        "  # try:\n",
        "    query_vector = image_data.loc[image_data['images_paths']==i]['features'].iloc[0]\n",
        "    df = SearchImage(image_data).search_by_vector(query_vector,5)\n",
        "    #df = pd.DataFrame()\n",
        "    df['Query Image'] = i[39:]\n",
        "    res_df = res_df.append(df,ignore_index = True)\n",
        "    stdout.write(\"\\r{2}% Completed --|-- Result Shape : {0} --|-- Errors: {1} \".format(res_df.shape, len(err),round((ind+1)*100/len(i_list),2)))\n",
        "    stdout.flush()\n",
        "    #print(i[39:],res_df.shape,len(err))\n",
        "    #res_df = df[df['Image Path'] != df['Query Image']]\n",
        "  # except:\n",
        "  #   k=k+1\n",
        "  #   err.append(i)\n",
        "  #   continue\n",
        "res_df.to_csv('hp_pmay_results.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "res_df = pd.DataFrame()\n",
        "for i in os.listdir('/content/drive/My Drive/hp_pmay_results_csv'):\n",
        "    df = pd.read_csv('/content/drive/My Drive/hp_pmay_results_csv/'+i)\n",
        "    res_df = res_df.append(df)\n",
        "res_df.to_csv('hp_pmay_results_final.csv')"
      ],
      "metadata": {
        "id": "SGDiVzBILC1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_df.shape"
      ],
      "metadata": {
        "id": "4S6vIPY6feg0",
        "outputId": "d9888e8c-f594-4e32-b65e-d953992ca2c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(226372, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of Image Similarity using Annoy",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}